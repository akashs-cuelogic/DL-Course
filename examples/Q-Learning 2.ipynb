{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named windy_gridworld",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f58a99aee69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwindy_gridworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named windy_gridworld"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import itertools \n",
    "import matplotlib \n",
    "import matplotlib.style \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "\n",
    "\n",
    "from collections import defaultdict \n",
    "from windy_gridworld import WindyGridworldEnv \n",
    "import plotting \n",
    "\n",
    "matplotlib.style.use('ggplot') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WindyGridworldEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-847c73303375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindyGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'WindyGridworldEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = WindyGridworldEnv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "\t\"\"\" \n",
    "\tCreates an epsilon-greedy policy based \n",
    "\ton a given Q-function and epsilon. \n",
    "\t\n",
    "\tReturns a function that takes the state \n",
    "\tas an input and returns the probabilities \n",
    "\tfor each action in the form of a numpy array \n",
    "\tof length of the action space(set of possible actions). \n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state): \n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions, \n",
    "\t\t\t\tdtype = float) * epsilon / num_actions \n",
    "\t\t\t\t\n",
    "\t\tbest_action = np.argmax(Q[state]) \n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon) \n",
    "\t\treturn Action_probabilities \n",
    "\n",
    "\treturn policyFunction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0, \n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1): \n",
    "\t\"\"\" \n",
    "\tQ-Learning algorithm: Off-policy TD control. \n",
    "\tFinds the optimal greedy policy while improving \n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\t\n",
    "\t# Action value function \n",
    "\t# A nested dictionary that maps \n",
    "\t# state -> (action -> action-value). \n",
    "\tQ = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "\n",
    "\t# Keeps track of useful statistics \n",
    "\tstats = plotting.EpisodeStats( \n",
    "\t\tepisode_lengths = np.zeros(num_episodes), \n",
    "\t\tepisode_rewards = np.zeros(num_episodes))\t \n",
    "\t\n",
    "\t# Create an epsilon greedy policy function \n",
    "\t# appropriately for environment action space \n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "\t\n",
    "\t# For every episode \n",
    "\tfor ith_episode in range(num_episodes): \n",
    "\t\t\n",
    "\t\t# Reset the environment and pick the first action \n",
    "\t\tstate = env.reset() \n",
    "\t\t\n",
    "\t\tfor t in itertools.count(): \n",
    "\t\t\t\n",
    "\t\t\t# get probabilities of all actions from current state \n",
    "\t\t\taction_probabilities = policy(state) \n",
    "\n",
    "\t\t\t# choose action according to \n",
    "\t\t\t# the probability distribution \n",
    "\t\t\taction = np.random.choice(np.arange( \n",
    "\t\t\t\t\tlen(action_probabilities)), \n",
    "\t\t\t\t\tp = action_probabilities) \n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state \n",
    "\t\t\tnext_state, reward, done, _ = env.step(action) \n",
    "\n",
    "\t\t\t# Update statistics \n",
    "\t\t\tstats.episode_rewards[i_episode] += reward \n",
    "\t\t\tstats.episode_lengths[i_episode] = t \n",
    "\t\t\t\n",
    "\t\t\t# TD Update \n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\t \n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "\t\t\ttd_delta = td_target - Q[state][action] \n",
    "\t\t\tQ[state][action] += alpha * td_delta \n",
    "\n",
    "\t\t\t# done is True if episode terminated \n",
    "\t\t\tif done: \n",
    "\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\tstate = next_state \n",
    "\t\n",
    "\treturn Q, stats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env, 1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_episode_stats(stats) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
